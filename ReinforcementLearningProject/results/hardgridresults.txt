25% 
Qlearn

Policy

Value

Num generated: 896; num unique: 74
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,2502,1319,334,1528,1570,813,1836,2351,651,3463,1317,1026,451,624,1247,2509,1283,831,700,903,1135,208,253,218,543,1043,3190,5362,307,3757,199,550,319,264,333,1842,188,1181,192,502,1893,3529,1346,608,549,1435,1420,1223,187,683,2223,278,257,2234,3328,734,311,269,3218,1042,1235,473,332,2819,356,1482,303,106,906,129,970,696,502,263,2587,1962,1022,2708,287,3555,529,845,785,1900,424,527,410,1870,2935,3218,2278,1046,1043,2196,843,1207,2434,1292,551,521
Policy Iteration,1844,4692,2011,723,1341,1079,455,1052,447,897,180,1282,1149,311,345,700,1215,2197,1359,2359,2228,436,321,261,1254,985,326,2327,1727,1229,1205,970,208,1255,1657,1377,135,1426,2860,607,1526,1316,990,3344,527,1488,529,478,940,315,817,278,153,450,3631,2511,1038,1091,218,272,836,1304,379,484,439,1381,1429,1153,531,917,214,637,752,691,933,1968,1343,2559,1805,770,1915,219,1326,947,1555,378,256,330,3495,1292,4061,3211,241,2542,2552,2285,2039,358,123,633
Q Learning,606,2202,4833,396,2032,995,784,1702,1404,118,370,929,2792,325,519,396,1029,2193,928,530,127,961,259,429,2466,201,430,503,215,488,3796,326,2544,258,646,356,495,974,776,1727,814,1084,2121,2860,1131,997,255,276,420,1207,2529,493,541,531,1208,221,486,1690,131,450,463,410,1593,783,1232,455,877,1972,1054,708,872,607,795,1064,1274,301,292,835,1614,1398,597,148,252,1345,228,466,1627,1216,607,3404,1334,412,532,745,1378,354,716,1032,2131,219

The data below shows the number of milliseconds the algorithm required to generate 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,57,5,3,4,4,5,5,10,10,6,6,6,14,14,9,8,9,10,11,12,12,10,13,12,14,12,14,14,15,17,15,17,18,19,18,20,22,21,22,21,21,41,22,25,25,22,24,27,24,24,24,25,26,26,49,27,32,30,36,35,35,36,32,31,36,32,33,33,36,35,37,43,43,63,39,42,40,38,39,39,43,42,38,43,40,42,44,47,47,45,49,47,54,67,58,53,50,65,51,49
Policy Iteration,7,6,5,7,10,11,16,14,17,18,18,19,24,22,28,22,27,33,38,43,46,45,55,50,64,59,81,63,57,62,61,70,67,65,70,70,70,79,80,85,83,134,83,89,86,90,93,86,101,107,107,110,105,100,103,113,107,118,112,120,117,115,121,123,127,134,123,132,138,140,134,153,154,154,154,146,149,168,173,165,199,188,176,173,186,179,160,140,169,196,196,200,202,199,208,191,207,208,210,202
Q Learning,26,17,30,20,37,15,22,22,46,31,36,24,45,29,48,53,42,57,60,42,80,57,53,110,84,66,79,101,56,81,100,98,125,95,121,82,72,124,84,118,108,106,134,103,174,110,138,122,154,127,163,142,175,133,149,152,196,141,172,204,172,214,205,222,217,214,205,191,166,216,232,214,173,250,219,176,213,203,237,256,234,264,339,331,359,353,400,419,361,429,321,500,380,376,396,413,422,410,591,571

The data below shows the total reward gained for 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration Rewards,-17646.0,-6761.0,-8746.0,-5287.0,-29584.0,-1800.0,-14307.0,-12842.0,-9261.0,-17617.0,-6066.0,-4884.0,-349.0,-7650.0,-5798.0,-7357.0,-5735.0,-10134.0,-2776.0,-3969.0,-6676.0,-106.0,-1636.0,-3383.0,-2520.0,-13613.0,-6355.0,-44068.0,-4363.0,-27811.0,-6136.0,-1240.0,-6256.0,-7785.0,-3993.0,-5700.0,-1769.0,-2366.0,-6723.0,-4261.0,-15948.0,-36493.0,-7778.0,-506.0,-10941.0,-11530.0,-8149.0,-19634.0,-481.0,-6917.0,-12219.0,-6413.0,-2729.0,-22724.0,-4612.0,-15878.0,-3377.0,-2543.0,-13115.0,-5197.0,-6380.0,-3440.0,-4982.0,-17765.0,-6986.0,-22962.0,-300.0,-2776.0,-2685.0,-1215.0,-2947.0,-7920.0,-6736.0,-1250.0,-18127.0,-12849.0,-6068.0,-30425.0,-7115.0,-8403.0,-10624.0,-9653.0,-8702.0,-23578.0,-5470.0,-425.0,-10604.0,-22558.0,-29167.0,-28361.0,-11185.0,-6092.0,-9158.0,-3678.0,-5196.0,-13183.0,-8470.0,-4655.0,-2924.0,-8339.0
Policy Iteration Rewards,-2831.0,-21915.0,-7255.0,-9432.0,-12228.0,-3650.0,-3323.0,-3227.0,-6879.0,-8022.0,-3642.0,-18703.0,-9660.0,-209.0,-243.0,-15646.0,-7251.0,-16648.0,-14523.0,-26413.0,-13907.0,-5482.0,-1704.0,-2436.0,-10854.0,-8605.0,-3986.0,-19451.0,-23504.0,-7760.0,-8627.0,-3244.0,-1690.0,-1153.0,-11158.0,-24243.0,-1320.0,-4987.0,-27211.0,-11098.0,-8057.0,-12599.0,-3858.0,-29774.0,-425.0,-1386.0,-6961.0,-15523.0,-7966.0,-3876.0,-3982.0,-10274.0,-1932.0,-5793.0,-26398.0,-13200.0,-11925.0,-21086.0,-2393.0,-1556.0,-11327.0,-2687.0,-871.0,-8104.0,-1327.0,-9694.0,-8653.0,-8377.0,-6270.0,-2399.0,-3379.0,-9346.0,-5600.0,-9499.0,-12018.0,-12261.0,-10547.0,-16416.0,-19622.0,-2054.0,-9337.0,-117.0,-22707.0,-8765.0,-8086.0,-7404.0,-4906.0,-2010.0,-13392.0,-1190.0,-25739.0,-31225.0,-1327.0,-17191.0,-25418.0,-9806.0,-12431.0,-6295.0,-1605.0,-5481.0
Q Learning Rewards,-2484.0,-13683.0,-39876.0,-5145.0,-5890.0,-893.0,-682.0,-12886.0,-13974.0,-4669.0,-3238.0,-1718.0,-12590.0,-1510.0,-3684.0,-16530.0,-1917.0,-4665.0,-7063.0,-527.0,-2005.0,-6106.0,-7879.0,-327.0,-18897.0,-6633.0,-10129.0,-5351.0,-1697.0,-7316.0,-7951.0,-3491.0,-11055.0,-3027.0,-3910.0,-2630.0,-14550.0,-5921.0,-674.0,-17762.0,-712.0,-13951.0,-27660.0,-14836.0,-17265.0,-13864.0,-3618.0,-3342.0,-3981.0,-11104.0,-29454.0,-4450.0,-11527.0,-4191.0,-13283.0,-1505.0,-9294.0,-1588.0,-29.0,-6189.0,-8578.0,-4466.0,-15549.0,-11868.0,-13703.0,-6392.0,-973.0,-19294.0,-4714.0,-14367.0,-9878.0,-4168.0,-15345.0,-10466.0,-6914.0,-2080.0,-2071.0,-733.0,-16065.0,-13869.0,-495.0,-3115.0,-150.0,-3718.0,-4680.0,-2443.0,-28255.0,-1906.0,-505.0,-32705.0,-2717.0,-1201.0,-7855.0,-25591.0,-17611.0,-5103.0,-4475.0,-1029.0,-12820.0,-1503.0


50% 
Qlearn

Policy

Value

Num generated: 896; num unique: 74
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,2440,16489,9934,2477,2115,3422,1336,20004,1702,4319,8585,948,885,561,1930,2132,465,1292,51,11391,76757,1942,1844,52098,72004,1952,15697,22847,57141,63237,19096,97609,2894,51862,54387,12471,60504,74752,215155,6550,88510,20299,2871,348,233,102,71,67,234,98,88,81,101,54,119,89,77,75,80,70,44,80,77,39,47,59,78,63,49,120,79,102,55,63,63,81,122,55,65,90,97,63,70,62,76,79,63,70,74,71,89,48,44,110,69,64,75,83,77,55
Policy Iteration,1958,2602,2491,6753,3333,1034,1311,2793,3634,1847,259,2888,6024,428,1397,1921,290,941,177,106,2388,542,457,991,149,7919,119,1345,175,8511,2301,6128,5117,15141,6249,108,1637,6077,6740,56,79,1930,1983,338,1120,282,72,76,80,75,45,81,68,60,44,73,90,47,108,89,39,111,96,79,65,67,68,63,71,29,59,97,54,54,96,136,50,68,89,86,58,40,41,55,51,65,47,73,62,83,98,112,102,63,136,48,43,65,129,54
Q Learning,279,1609,1851,137,394,855,811,815,2604,1505,271,501,906,351,304,353,982,298,162,496,187,1190,642,124,701,780,131,373,1194,533,225,1163,638,148,303,116,304,346,248,325,174,317,232,123,530,334,524,679,116,1025,221,175,133,147,155,245,331,465,174,101,115,69,85,244,215,372,337,192,847,825,149,452,516,1461,149,450,324,1049,76,233,217,96,90,54,146,62,528,145,213,132,787,230,94,668,499,303,88,470,192,145

The data below shows the number of milliseconds the algorithm required to generate 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,38,8,2,6,3,4,5,4,5,6,12,7,7,7,9,8,10,11,10,10,10,11,11,11,13,12,12,36,14,14,14,15,15,17,17,17,21,21,18,38,32,37,36,36,37,32,39,37,38,44,39,59,41,41,40,42,46,48,44,46,51,52,51,51,48,52,52,56,51,55,54,58,61,55,57,60,59,58,63,63,62,64,66,70,62,68,69,63,66,73,68,70,74,80,69,75,73,74,71,79
Policy Iteration,8,6,5,6,7,9,10,11,11,12,13,14,16,15,16,19,19,21,20,23,23,25,25,29,26,27,27,29,32,31,32,33,35,41,35,40,41,41,44,51,43,46,48,47,48,49,51,52,52,53,58,62,56,60,57,62,64,62,64,64,67,88,69,71,70,72,74,73,76,72,78,76,80,80,79,81,84,85,83,87,89,88,93,90,94,92,91,98,99,98,99,97,97,102,109,109,105,108,105,112
Q Learning,24,13,13,25,22,18,34,20,22,21,20,23,19,20,27,16,37,39,34,25,34,31,37,28,47,46,31,27,43,38,43,40,42,50,49,54,43,48,53,40,46,36,40,38,53,51,62,56,53,56,49,62,63,59,62,82,54,59,84,75,90,99,74,68,65,68,94,69,83,83,71,67,77,93,80,72,100,79,64,102,78,114,91,81,88,87,87,96,84,88,90,97,105,94,91,95,110,106,117,101

The data below shows the total reward gained for 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration Rewards,-4912.0,-17278.0,-11416.0,-2375.0,-2013.0,-4013.0,-2422.0,-21783.0,-1600.0,-4217.0,-8483.0,-846.0,-783.0,-459.0,-1828.0,-2030.0,-363.0,-1190.0,51.0,-12576.0,-77051.0,-1840.0,-1742.0,-52590.0,-72298.0,-2147.0,-16387.0,-22745.0,-57039.0,-64125.0,-18994.0,-97705.0,-2792.0,-51760.0,-55275.0,-12369.0,-60699.0,-75046.0,-216439.0,-6448.0,-89101.0,-20593.0,-2769.0,-246.0,-131.0,0.0,31.0,35.0,-330.0,4.0,14.0,21.0,1.0,48.0,-17.0,13.0,25.0,27.0,22.0,32.0,58.0,22.0,25.0,63.0,55.0,43.0,24.0,39.0,53.0,-18.0,23.0,-99.0,47.0,39.0,-60.0,21.0,-20.0,47.0,37.0,12.0,5.0,39.0,32.0,40.0,26.0,23.0,39.0,-67.0,28.0,31.0,13.0,54.0,58.0,-8.0,33.0,38.0,27.0,19.0,25.0,47.0
Policy Iteration Rewards,-1856.0,-2698.0,-3082.0,-7641.0,-3528.0,-932.0,-1209.0,-2691.0,-3532.0,-1745.0,-157.0,-3281.0,-5922.0,-524.0,-1295.0,-1819.0,-188.0,-839.0,-75.0,-4.0,-2286.0,-440.0,-355.0,-889.0,-47.0,-7916.0,-116.0,-1243.0,-73.0,-8904.0,-2199.0,-6026.0,-5015.0,-15039.0,-6147.0,-6.0,-1535.0,-6173.0,-6638.0,46.0,23.0,-1828.0,-1881.0,-236.0,-1414.0,-180.0,30.0,26.0,22.0,27.0,-240.0,21.0,34.0,42.0,58.0,-268.0,-186.0,55.0,-6.0,13.0,63.0,-9.0,6.0,23.0,37.0,35.0,34.0,-1248.0,31.0,73.0,43.0,5.0,48.0,48.0,6.0,-34.0,52.0,34.0,13.0,16.0,44.0,62.0,61.0,47.0,51.0,37.0,55.0,29.0,40.0,19.0,4.0,-10.0,0.0,39.0,-430.0,54.0,59.0,37.0,-27.0,48.0
Q Learning Rewards,-4731.0,-9823.0,-4125.0,-3896.0,-1480.0,-753.0,-5461.0,-713.0,-4284.0,-1403.0,-1852.0,-1191.0,-804.0,-2328.0,-2380.0,-251.0,-880.0,-1186.0,-159.0,-6631.0,-85.0,-3860.0,-540.0,-22.0,-698.0,-678.0,-29.0,-3736.0,-1092.0,-1421.0,-123.0,-3239.0,-2813.0,-937.0,-201.0,-707.0,-1885.0,-10243.0,-839.0,-916.0,-1854.0,-215.0,-922.0,-21.0,-428.0,-529.0,-521.0,-577.0,-2588.0,-3497.0,-119.0,-73.0,-31.0,-45.0,-53.0,-143.0,-229.0,-3432.0,-666.0,-395.0,-2191.0,-264.0,-1567.0,-1330.0,-1202.0,-270.0,-235.0,-585.0,-6685.0,-6168.0,-47.0,-1439.0,-8037.0,-1359.0,-47.0,-348.0,-222.0,-947.0,-766.0,-131.0,-115.0,-390.0,12.0,-546.0,-1529.0,-554.0,-426.0,-43.0,-111.0,-30.0,-685.0,-920.0,-685.0,-764.0,-397.0,-201.0,14.0,-368.0,-3951.0,-2518.0


75%
Qlearn

Policy

Value

Num generated: 896; num unique: 74
//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,3910,31615,26765,1249545,312528,89414,89751,35814,135787,53382,156614,43184,1038,1928,115,31,42,31,37,42,32,25,28,41,49,31,33,38,37,27,47,32,33,39,27,33,34,28,31,39,31,32,37,31,46,44,26,33,38,37,29,39,41,35,31,28,26,31,34,31,30,32,49,32,33,33,36,28,33,42,26,38,31,33,31,36,52,35,34,42,31,27,41,39,32,39,35,36,27,37,33,34,31,42,38,25,36,37,34,39
Policy Iteration,10305,47063,74694,50733,14821,30279,15315,7797,31670,96187,54358,1209,42695,3394,66,2573,35,28,37,30,46,45,38,29,30,25,41,39,29,35,35,46,32,30,40,36,37,31,35,42,31,40,42,42,29,45,32,41,36,40,36,40,35,26,33,33,35,37,32,29,51,27,39,36,33,37,33,28,28,32,30,31,44,38,36,43,34,32,40,34,24,40,52,33,34,33,27,37,35,32,35,43,33,35,41,33,27,31,34,29
Q Learning,682,867,472,326,752,302,110,650,135,493,360,588,104,69,78,182,75,147,297,76,94,154,131,53,36,53,58,53,84,53,126,398,51,67,80,38,76,46,196,43,59,68,81,37,97,71,69,51,61,251,67,136,147,46,49,225,75,127,48,107,89,64,35,94,60,107,194,51,51,41,135,178,84,243,117,63,52,81,258,51,60,42,143,92,52,54,133,103,91,75,68,74,93,143,226,248,34,75,50,89

The data below shows the number of milliseconds the algorithm required to generate 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,41,6,4,6,3,4,4,5,5,6,6,7,7,7,7,8,9,9,10,10,12,11,12,12,14,13,14,14,15,17,16,16,17,17,19,18,19,20,20,20,21,21,23,22,23,22,23,23,23,25,100,48,47,42,27,29,28,29,28,30,29,29,29,33,33,32,32,33,35,34,34,36,37,36,37,37,40,38,40,39,42,78,42,42,42,42,45,43,46,46,44,46,45,53,49,46,48,49,48,54
Policy Iteration,6,6,4,5,7,8,9,9,10,12,13,13,14,16,17,23,29,27,29,29,29,35,34,36,36,42,38,33,34,32,34,52,35,38,37,40,39,42,45,44,47,53,48,45,47,50,52,57,58,57,60,61,57,58,78,63,61,61,65,64,65,66,68,89,85,74,71,70,71,73,74,82,84,77,79,85,89,87,86,84,84,88,91,88,90,89,95,96,98,106,102,104,103,101,105,109,106,106,106,107
Q Learning,16,7,20,9,7,10,13,14,12,13,16,17,20,18,17,18,15,18,18,16,14,18,17,17,15,18,19,18,16,22,18,25,24,21,15,21,21,19,17,24,21,20,23,27,26,24,24,24,22,25,23,30,31,28,29,29,30,28,27,27,23,28,22,25,28,28,29,28,36,27,28,39,32,43,30,29,30,30,31,28,34,32,43,36,33,36,39,28,35,26,26,31,35,28,26,31,34,32,27,39

The data below shows the total reward gained for 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration Rewards,-4204.0,-34186.0,-28049.0,-1249839.0,-312723.0,-89312.0,-90045.0,-35712.0,-135685.0,-53280.0,-156512.0,-43082.0,-936.0,-1826.0,-13.0,71.0,60.0,71.0,65.0,60.0,70.0,77.0,74.0,61.0,53.0,71.0,69.0,64.0,65.0,75.0,55.0,70.0,69.0,63.0,75.0,69.0,68.0,74.0,71.0,63.0,71.0,70.0,65.0,71.0,56.0,58.0,76.0,69.0,64.0,-133.0,73.0,63.0,61.0,67.0,71.0,74.0,76.0,71.0,68.0,71.0,72.0,70.0,53.0,70.0,69.0,69.0,66.0,74.0,69.0,60.0,76.0,64.0,71.0,69.0,71.0,66.0,50.0,67.0,68.0,60.0,71.0,75.0,61.0,63.0,70.0,63.0,67.0,66.0,75.0,65.0,69.0,68.0,71.0,60.0,64.0,77.0,66.0,65.0,68.0,63.0
Policy Iteration Rewards,-11985.0,-48446.0,-74592.0,-50730.0,-15412.0,-30375.0,-15411.0,-7695.0,-31568.0,-96085.0,-54553.0,-1107.0,-42593.0,-3292.0,36.0,-2471.0,67.0,74.0,65.0,72.0,56.0,57.0,64.0,73.0,72.0,77.0,61.0,63.0,73.0,67.0,67.0,56.0,70.0,72.0,62.0,66.0,65.0,71.0,67.0,60.0,71.0,62.0,60.0,60.0,73.0,57.0,70.0,61.0,66.0,62.0,66.0,62.0,67.0,76.0,69.0,69.0,67.0,65.0,70.0,73.0,51.0,75.0,63.0,66.0,69.0,65.0,69.0,74.0,74.0,70.0,72.0,71.0,58.0,64.0,66.0,59.0,68.0,70.0,62.0,68.0,78.0,62.0,50.0,69.0,68.0,69.0,75.0,65.0,67.0,70.0,67.0,59.0,69.0,67.0,61.0,69.0,75.0,71.0,68.0,73.0

