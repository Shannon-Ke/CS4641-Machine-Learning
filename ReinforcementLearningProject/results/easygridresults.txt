0.25 stochasticity report:

	Num generated: 196; num unique: 17
	//Aggregate Analysis//

	The data below shows the number of steps/actions the agent required to reach 
	the terminal state given the number of iterations the algorithm was run.
	Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
	Value Iteration,57,1675,127,174,348,255,191,98,31,702,174,863,54,60,1853,171,373,187,309,478,203,143,1335,471,422,788,188,333,690,306,31,517,1146,25,159,339,877,653,958,630,363,166,138,807,411,402,37,57,656,481,158,47,81,159,210,102,368,68,71,265,146,383,83,233,570,1470,75,1107,558,167,355,394,253,1221,235,342,80,43,441,394,54,690,77,1758,34,118,1045,262,717,258,484,437,130,226,658,36,274,284,465,119
	Policy Iteration,95,198,184,93,150,295,147,28,416,120,703,120,1801,157,324,883,132,219,811,722,224,261,263,579,673,913,245,194,180,110,76,28,124,32,435,592,42,845,172,76,437,56,175,290,293,207,98,419,167,40,134,241,194,30,269,23,227,232,502,183,149,23,271,470,903,323,197,395,604,101,56,58,827,347,311,705,23,57,161,271,90,545,49,454,843,132,329,56,1120,662,123,120,37,323,80,355,1128,106,222,39
	Q Learning,419,103,249,224,210,134,155,411,143,353,121,1080,976,213,467,263,294,118,570,472,296,48,715,54,835,640,207,503,426,347,68,120,448,1435,235,329,139,101,97,170,318,101,366,441,1388,78,437,78,306,504,342,124,494,317,239,198,1514,153,92,382,116,347,45,248,303,157,412,58,167,232,88,48,383,42,465,284,253,157,521,69,179,121,463,355,101,85,175,223,192,317,82,660,267,228,242,220,293,935,879,830

	The data below shows the number of milliseconds the algorithm required to generate 
	the optimal policy given the number of iterations the algorithm was run.
	Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
	Value Iteration,34,8,2,2,2,3,3,4,3,4,5,6,12,5,5,6,6,13,6,6,7,7,8,6,10,9,8,11,11,7,6,7,5,5,5,8,8,8,6,8,6,8,9,6,10,6,9,9,16,15,13,15,15,13,13,14,15,16,13,11,12,13,12,13,13,10,8,9,13,9,11,12,10,9,17,10,10,10,17,10,10,11,11,12,11,18,26,26,27,30,30,31,30,31,31,31,32,24,22,16
	Policy Iteration,4,2,5,3,4,4,16,5,6,8,8,10,11,9,11,13,12,12,13,15,14,15,18,16,18,19,18,19,18,19,24,19,16,20,14,13,16,19,15,20,18,15,21,18,19,21,19,27,25,24,33,40,44,35,46,20,24,23,28,27,26,26,34,32,36,23,24,22,24,30,25,25,25,25,29,28,41,34,29,36,32,38,35,35,35,36,29,30,31,33,32,31,30,31,34,41,35,38,35,37
	Q Learning,20,4,16,10,11,18,32,16,16,21,32,20,29,17,21,31,21,25,26,15,30,24,31,21,26,29,35,23,29,37,25,34,28,37,28,34,41,44,35,38,41,41,36,41,39,51,43,44,36,45,40,45,57,62,57,36,65,56,53,65,64,64,56,66,61,65,61,75,67,78,56,55,60,81,68,82,70,81,81,67,83,74,70,79,66,86,79,84,75,84,92,91,96,87,90,94,93,81,99,127

	The data below shows the total reward gained for 
	the optimal policy given the number of iterations the algorithm was run.
	Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
	Value Iteration Rewards,45.0,-1573.0,-25.0,-72.0,-246.0,-153.0,-89.0,4.0,71.0,-600.0,-72.0,-761.0,48.0,42.0,-1751.0,-69.0,-271.0,-85.0,-207.0,-376.0,-101.0,-41.0,-1233.0,-369.0,-320.0,-686.0,-86.0,-231.0,-588.0,-204.0,71.0,-415.0,-1044.0,77.0,-57.0,-237.0,-775.0,-551.0,-856.0,-528.0,-261.0,-64.0,-36.0,-705.0,-309.0,-300.0,65.0,45.0,-554.0,-379.0,-56.0,55.0,21.0,-57.0,-108.0,0.0,-266.0,34.0,31.0,-163.0,-44.0,-281.0,19.0,-131.0,-468.0,-1368.0,27.0,-1005.0,-456.0,-65.0,-253.0,-292.0,-151.0,-1119.0,-133.0,-240.0,22.0,59.0,-339.0,-292.0,48.0,-588.0,25.0,-1656.0,68.0,-16.0,-943.0,-160.0,-615.0,-156.0,-382.0,-335.0,-28.0,-124.0,-556.0,66.0,-172.0,-182.0,-363.0,-17.0
	Policy Iteration Rewards,7.0,-96.0,-82.0,9.0,-48.0,-193.0,-45.0,74.0,-314.0,-18.0,-601.0,-18.0,-1699.0,-55.0,-222.0,-781.0,-30.0,-117.0,-709.0,-620.0,-122.0,-159.0,-161.0,-477.0,-571.0,-811.0,-143.0,-92.0,-78.0,-8.0,26.0,74.0,-22.0,70.0,-333.0,-490.0,60.0,-743.0,-70.0,26.0,-335.0,46.0,-73.0,-188.0,-191.0,-105.0,4.0,-317.0,-65.0,62.0,-32.0,-139.0,-92.0,72.0,-167.0,79.0,-125.0,-130.0,-400.0,-81.0,-47.0,79.0,-169.0,-368.0,-801.0,-221.0,-95.0,-293.0,-502.0,1.0,46.0,44.0,-725.0,-245.0,-209.0,-603.0,79.0,45.0,-59.0,-169.0,12.0,-443.0,53.0,-352.0,-741.0,-30.0,-227.0,46.0,-1018.0,-560.0,-21.0,-18.0,65.0,-221.0,22.0,-253.0,-1026.0,-4.0,-120.0,63.0

0.5 report
	Num generated: 196; num unique: 17
	//Aggregate Analysis//

	The data below shows the number of steps/actions the agent required to reach 
	the terminal state given the number of iterations the algorithm was run.
	Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
	Value Iteration,58,127,20,42,31,18,24,25,22,18,22,21,11,17,16,30,48,19,29,15,15,15,17,18,39,13,24,25,35,21,13,16,22,15,47,33,38,32,40,43,16,44,22,72,13,11,20,13,19,42,12,25,21,32,21,21,33,43,23,15,26,23,45,37,29,20,39,23,24,37,16,25,67,12,38,20,17,40,26,22,16,23,17,36,38,18,19,33,25,49,28,17,23,35,19,13,14,40,33,30
	Policy Iteration,124,23,34,20,38,35,47,17,43,24,18,30,28,21,28,16,14,22,17,20,15,23,48,25,21,23,10,65,11,23,32,22,14,13,40,30,41,82,18,38,21,52,16,28,18,37,22,27,20,15,17,19,11,61,19,13,13,16,12,24,22,18,31,19,33,18,38,24,37,43,22,28,42,23,15,19,33,46,29,19,27,25,28,52,25,9,16,48,36,17,35,47,13,13,36,18,11,46,17,15
	Q Learning,179,245,254,145,323,105,34,601,222,39,72,194,43,20,112,425,18,47,32,19,12,164,129,206,120,80,91,20,64,79,81,106,215,84,62,125,140,202,56,197,24,113,12,55,52,234,811,294,67,56,21,88,458,61,49,177,18,15,45,34,211,75,63,26,14,38,38,238,64,42,31,80,15,187,55,34,52,30,97,150,40,41,29,22,139,24,53,72,75,35,28,24,38,30,328,22,48,29,42,38

	The data below shows the number of milliseconds the algorithm required to generate 
	the optimal policy given the number of iterations the algorithm was run.
	Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
	Value Iteration,37,9,3,6,4,7,6,14,5,6,6,6,6,7,6,7,13,7,7,7,6,5,8,14,10,9,10,8,11,7,7,12,10,9,6,15,17,21,12,19,26,13,13,14,27,16,18,16,12,12,12,12,12,19,21,22,22,23,27,10,8,13,13,10,10,15,16,15,13,14,16,10,10,11,11,19,17,13,16,19,20,27,25,21,25,25,23,30,36,30,24,22,20,14,20,26,22,21,22,18
	Policy Iteration,4,2,4,4,6,5,6,6,7,9,9,9,10,12,45,21,13,13,14,29,17,18,18,16,18,18,20,21,19,18,22,23,24,17,20,27,36,38,42,42,46,47,39,80,33,29,35,37,34,34,35,36,38,38,38,38,38,39,47,44,47,46,45,51,49,51,67,52,52,48,51,49,48,49,64,80,79,80,89,76,82,53,57,64,54,65,55,55,55,57,59,57,61,60,85,66,60,65,69,69
	Q Learning,12,4,11,15,16,12,11,41,11,15,11,21,12,15,42,44,14,19,43,40,30,25,37,26,32,32,29,36,17,22,31,29,41,31,16,18,25,23,30,27,32,24,17,31,18,41,34,35,30,25,29,31,33,37,29,37,35,32,36,17,17,17,16,20,17,18,27,22,22,22,21,30,22,23,21,22,21,25,22,27,20,22,24,27,25,27,30,24,21,24,20,24,26,31,29,22,35,24,30,27

	The data below shows the total reward gained for 
	the optimal policy given the number of iterations the algorithm was run.
	Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
	Value Iteration Rewards,44.0,-25.0,82.0,60.0,71.0,84.0,78.0,77.0,80.0,84.0,80.0,81.0,91.0,85.0,86.0,72.0,54.0,83.0,73.0,87.0,87.0,87.0,85.0,84.0,63.0,89.0,78.0,77.0,67.0,81.0,89.0,86.0,80.0,87.0,55.0,69.0,64.0,70.0,62.0,59.0,86.0,58.0,80.0,30.0,89.0,91.0,82.0,89.0,83.0,60.0,90.0,77.0,81.0,70.0,81.0,81.0,69.0,59.0,79.0,87.0,76.0,79.0,57.0,65.0,73.0,82.0,63.0,79.0,78.0,65.0,86.0,77.0,35.0,90.0,64.0,82.0,85.0,62.0,76.0,80.0,86.0,79.0,85.0,66.0,64.0,84.0,83.0,69.0,77.0,53.0,74.0,85.0,79.0,67.0,83.0,89.0,88.0,62.0,69.0,72.0
	Policy Iteration Rewards,-22.0,79.0,68.0,82.0,64.0,67.0,55.0,85.0,59.0,78.0,84.0,72.0,74.0,81.0,74.0,86.0,88.0,80.0,85.0,82.0,87.0,79.0,54.0,77.0,81.0,79.0,92.0,37.0,91.0,79.0,70.0,80.0,88.0,89.0,62.0,72.0,61.0,20.0,84.0,64.0,81.0,50.0,86.0,74.0,84.0,65.0,80.0,75.0,82.0,87.0,85.0,83.0,91.0,41.0,83.0,89.0,89.0,86.0,90.0,78.0,80.0,84.0,71.0,83.0,69.0,84.0,64.0,78.0,65.0,59.0,80.0,74.0,60.0,79.0,87.0,83.0,69.0,56.0,73.0,83.0,75.0,77.0,74.0,50.0,77.0,93.0,86.0,54.0,66.0,85.0,67.0,55.0,89.0,89.0,66.0,84.0,91.0,56.0,85.0,87.0


0.75 report

	Num generated: 196; num unique: 17
	//Aggregate Analysis//

	The data below shows the number of steps/actions the agent required to reach 
	the terminal state given the number of iterations the algorithm was run.
	Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
	Value Iteration,109,481,18,11,13,18,10,12,13,12,9,12,12,10,15,11,13,13,14,16,14,15,22,12,13,17,18,9,13,12,14,24,14,11,11,13,9,10,14,20,16,9,15,11,14,20,13,13,9,17,15,10,15,14,9,13,9,13,10,13,11,22,11,10,12,11,13,12,9,10,15,15,15,13,13,11,11,21,13,15,17,16,9,15,15,15,12,11,9,13,10,11,13,15,13,10,13,10,11,20
	Policy Iteration,378,223,22,14,14,13,12,12,16,9,10,14,12,14,21,10,15,12,9,17,10,15,13,12,12,10,12,15,27,18,11,15,12,9,14,10,16,10,10,12,19,10,18,13,9,16,9,17,15,11,16,22,14,12,11,13,10,16,9,11,11,12,13,10,13,10,12,9,11,10,10,9,9,10,15,11,11,10,15,24,13,18,17,15,14,16,13,10,18,10,11,15,9,16,12,11,11,17,9,9
	Q Learning,72,29,45,26,47,14,28,53,23,15,17,16,17,10,26,29,22,13,13,9,45,12,71,29,13,20,12,9,24,25,11,14,36,10,18,25,22,14,26,40,11,16,20,11,38,73,16,102,29,22,23,38,78,95,17,17,21,10,12,9,15,65,34,11,17,34,11,38,11,38,20,30,23,12,20,16,23,16,16,23,36,49,10,20,10,138,41,13,22,9,53,17,17,12,20,33,13,30,104,14

	The data below shows the number of milliseconds the algorithm required to generate 
	the optimal policy given the number of iterations the algorithm was run.
	Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
	Value Iteration,39,5,3,13,7,4,4,5,4,4,5,5,5,5,3,11,6,7,6,7,7,8,9,5,6,8,9,11,15,9,10,10,10,11,10,15,15,17,16,8,7,8,8,19,15,19,13,11,11,11,9,7,7,7,11,16,17,19,20,26,13,13,13,15,8,9,9,9,9,14,13,12,10,9,10,12,14,16,14,10,19,14,17,27,23,21,22,28,29,31,27,18,19,12,13,17,19,18,19,14
	Policy Iteration,4,1,4,3,4,5,6,6,6,6,7,8,11,9,11,12,13,13,17,24,17,17,15,15,12,13,12,16,15,18,15,11,17,17,15,14,12,16,24,32,37,31,31,50,39,41,39,44,44,28,28,27,31,27,28,27,25,34,24,30,28,19,29,30,26,25,16,17,25,21,29,33,29,27,20,27,20,22,58,53,45,42,50,236,723,34,38,35,40,39,42,46,49,45,50,66,46,50,87,50
	Q Learning,16,5,4,7,3,5,8,4,6,8,11,3,28,8,6,7,4,8,8,8,5,5,9,9,5,6,21,6,6,9,5,6,9,11,9,7,9,10,15,11,7,9,18,7,9,9,6,10,10,9,8,13,14,11,11,19,11,11,7,10,9,7,6,7,10,8,6,15,6,7,11,8,10,9,8,11,11,7,8,7,8,7,10,8,9,7,8,8,8,7,7,7,9,7,7,9,7,8,11,7

	The data below shows the total reward gained for 
	the optimal policy given the number of iterations the algorithm was run.
	Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
	Value Iteration Rewards,-7.0,-379.0,84.0,91.0,89.0,84.0,92.0,90.0,89.0,90.0,93.0,90.0,90.0,92.0,87.0,91.0,89.0,89.0,88.0,86.0,88.0,87.0,80.0,90.0,89.0,85.0,84.0,93.0,89.0,90.0,88.0,78.0,88.0,91.0,91.0,89.0,93.0,92.0,88.0,82.0,86.0,93.0,87.0,91.0,88.0,82.0,89.0,89.0,93.0,85.0,87.0,92.0,87.0,88.0,93.0,89.0,93.0,89.0,92.0,89.0,91.0,80.0,91.0,92.0,90.0,91.0,89.0,90.0,93.0,92.0,87.0,87.0,87.0,89.0,89.0,91.0,91.0,81.0,89.0,87.0,85.0,86.0,93.0,87.0,87.0,87.0,90.0,91.0,93.0,89.0,92.0,91.0,89.0,87.0,89.0,92.0,89.0,92.0,91.0,82.0
	Policy Iteration Rewards,-276.0,-121.0,80.0,88.0,88.0,89.0,90.0,90.0,86.0,93.0,92.0,88.0,90.0,88.0,81.0,92.0,87.0,90.0,93.0,85.0,92.0,87.0,89.0,90.0,90.0,92.0,90.0,87.0,75.0,84.0,91.0,87.0,90.0,93.0,88.0,92.0,86.0,92.0,92.0,90.0,83.0,92.0,84.0,89.0,93.0,86.0,93.0,85.0,87.0,91.0,86.0,80.0,88.0,90.0,91.0,89.0,92.0,86.0,93.0,91.0,91.0,90.0,89.0,92.0,89.0,92.0,90.0,93.0,91.0,92.0,92.0,93.0,93.0,92.0,87.0,91.0,91.0,92.0,87.0,78.0,89.0,84.0,85.0,87.0,88.0,86.0,89.0,92.0,84.0,92.0,91.0,87.0,93.0,86.0,90.0,91.0,91.0,85.0,93.0,93.0
	Q Learning Rewards,-118.0,58.0,-88.0,-118.0,88.0,72.0,88.0,6.0,71.0,-95.0,84.0,6.0,72.0,75.0,78.0,90.0,74.0,75.0,75.0,59.0,88.0,88.0,89.0,88.0,91.0,82.0,77.0,45.0,82.0,84.0,78.0,73.0,74.0,88.0,63.0,87.0,85.0,87.0,90.0,92.0,63.0,-82.0,71.0,88.0,67.0,93.0,82.0,85.0,74.0,81.0,92.0,91.0,92.0,81.0,87.0,77.0,67.0,84.0,87.0,86.0,79.0,89.0,42.0,92.0,91.0,93.0,83.0,77.0,28.0,51.0,83.0,42.0,93.0,91.0,67.0,72.0,88.0,84.0,93.0,92.0,55.0,93.0,85.0,87.0,86.0,93.0,90.0,86.0,91.0,78.0,76.0,92.0,91.0,68.0,88.0,84.0,93.0,88.0,68.0,88.0
	

0.95 report

	Num generated: 196; num unique: 17
	//Aggregate Analysis//

	The data below shows the number of steps/actions the agent required to reach 
	the terminal state given the number of iterations the algorithm was run.
	Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
	Value Iteration,1581,381,9,11,9,9,9,9,10,9,9,9,12,11,10,11,9,9,9,9,10,9,9,10,9,9,9,11,9,12,9,9,9,11,9,10,9,10,9,9,9,9,12,9,9,9,9,9,9,9,10,12,9,9,9,9,11,11,9,9,9,9,9,9,9,11,9,10,10,11,11,9,9,10,9,9,11,9,9,10,9,12,10,9,9,9,9,9,9,11,9,9,10,10,10,9,9,11,9,9
	Policy Iteration,6491,6405,49,9,9,9,11,10,10,10,11,9,9,12,9,9,9,9,9,9,9,9,9,9,10,12,10,9,9,9,10,10,9,9,9,9,11,9,9,9,9,10,12,9,9,9,11,9,9,11,9,9,9,11,9,9,9,9,9,9,11,9,10,9,12,11,9,9,9,9,9,9,9,13,10,9,9,9,9,9,9,9,10,9,9,9,11,12,9,9,11,9,10,9,9,9,11,9,9,9
	Q Learning,87,118,117,200,25,49,16,14,16,12,11,10,11,13,14,13,28,31,11,10,10,11,11,12,14,69,10,112,9,9,11,12,9,9,11,9,11,9,10,9,10,9,12,11,11,12,19,12,9,15,9,11,12,19,10,9,9,9,9,11,14,18,10,9,11,9,10,10,10,15,10,9,11,10,12,12,10,11,10,14,9,10,16,14,11,10,9,9,10,11,23,9,10,9,10,9,9,11,10,14

	The data below shows the number of milliseconds the algorithm required to generate 
	the optimal policy given the number of iterations the algorithm was run.
	Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
	Value Iteration,108,1,1,2,2,2,7,3,3,4,4,4,5,5,7,7,7,10,6,6,7,8,8,8,14,8,9,8,8,9,10,7,13,14,15,17,7,10,12,12,13,13,10,17,12,11,14,14,10,9,9,16,14,12,12,12,14,17,11,7,7,7,8,8,8,8,8,8,8,10,8,9,9,9,9,9,9,9,9,10,15,18,17,18,18,18,21,21,19,15,13,11,11,11,12,12,12,12,12,13
	Policy Iteration,4,1,3,2,2,5,4,4,4,5,5,6,5,6,10,18,5,8,6,7,11,11,11,17,20,19,14,14,17,16,20,19,20,18,25,18,10,11,15,15,14,13,16,13,11,11,14,21,23,21,17,20,17,14,15,15,18,17,19,18,15,15,17,19,17,16,20,19,16,17,18,25,36,35,42,37,36,37,32,25,25,22,21,24,25,21,21,27,23,23,35,28,27,35,34,31,28,29,25,28
	Q Learning,11,4,4,9,2,3,3,3,4,3,3,4,6,5,2,2,6,3,3,3,5,3,3,3,3,3,3,4,3,3,2,2,2,2,2,2,2,2,2,2,3,2,5,13,3,2,2,2,3,3,2,2,2,3,5,4,4,5,9,5,3,3,3,3,5,5,10,5,8,6,5,6,5,6,9,5,5,6,6,5,4,3,3,5,3,6,4,4,3,8,4,5,4,6,7,7,9,8,4,3

	The data below shows the total reward gained for 
	the optimal policy given the number of iterations the algorithm was run.
	Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
	Value Iteration Rewards,-1479.0,-279.0,93.0,91.0,93.0,93.0,93.0,93.0,92.0,93.0,93.0,93.0,90.0,91.0,92.0,91.0,93.0,93.0,93.0,93.0,92.0,93.0,93.0,92.0,93.0,93.0,93.0,91.0,93.0,90.0,93.0,93.0,93.0,91.0,93.0,92.0,93.0,92.0,93.0,93.0,93.0,93.0,90.0,93.0,93.0,93.0,93.0,93.0,93.0,93.0,92.0,90.0,93.0,93.0,93.0,93.0,91.0,91.0,93.0,93.0,93.0,93.0,93.0,93.0,93.0,91.0,93.0,92.0,92.0,91.0,91.0,93.0,93.0,92.0,93.0,93.0,91.0,93.0,93.0,92.0,93.0,90.0,92.0,93.0,93.0,93.0,93.0,93.0,93.0,91.0,93.0,93.0,92.0,92.0,92.0,93.0,93.0,91.0,93.0,93.0
	Policy Iteration Rewards,-6389.0,-6303.0,53.0,93.0,93.0,93.0,91.0,92.0,92.0,92.0,91.0,93.0,93.0,90.0,93.0,93.0,93.0,93.0,93.0,93.0,93.0,93.0,93.0,93.0,92.0,90.0,92.0,93.0,93.0,93.0,92.0,92.0,93.0,93.0,93.0,93.0,91.0,93.0,93.0,93.0,93.0,92.0,90.0,93.0,93.0,93.0,91.0,93.0,93.0,91.0,93.0,93.0,93.0,91.0,93.0,93.0,93.0,93.0,93.0,93.0,91.0,93.0,92.0,93.0,90.0,91.0,93.0,93.0,93.0,93.0,93.0,93.0,93.0,89.0,92.0,93.0,93.0,93.0,93.0,93.0,93.0,93.0,92.0,93.0,93.0,93.0,91.0,90.0,93.0,93.0,91.0,93.0,92.0,93.0,93.0,93.0,91.0,93.0,93.0,93.0
